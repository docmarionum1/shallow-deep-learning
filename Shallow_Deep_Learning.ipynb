{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NyP15JthKd8_",
    "outputId": "b5a5eb2c-d9a5-4e56-9ead-0ef6afd1d89c"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Add, Dense, Input, LSTM\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda, TimeDistributed, RepeatVector, BatchNormalization, Reshape\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,  CSVLogger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq-C6fycGwNq"
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "model_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZSjGbxhLKSE"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Delete old runs and download data\n",
    "!rm *.hdf5\n",
    "!wget -O dataset.zip https://dl.dropboxusercontent.com/s/lc9es5lce77bl5l/SCUT-FBP5500_v2.1.zip?dl=0\n",
    "!wget -O contestants.zip https://dl.dropboxusercontent.com/s/dbw1tpt8f4mkass/contestants.zip?dl=0\n",
    "!unzip dataset.zip\n",
    "!unzip contestants.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fPVby7R19qDe",
    "outputId": "7402349d-a7a9-47f1-cf9b-ffc33d5d136f"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2XWJMHSaSLTG",
    "outputId": "49cfbb7d-3206-4014-c46a-af8df33d1928"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the training/testing data\n",
    "df = pd.read_excel('SCUT-FBP5500_v2/All_Ratings.xlsx')\n",
    "ratings = df[df['Filename'].str.contains('F')].groupby('Filename').mean()['Rating']\n",
    "train_df = ratings.sort_values().reset_index()\n",
    "train_df['Filename'] = 'SCUT-FBP5500_v2/Images/' + train_df['Filename'] \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CPXbmKq2F4Q7",
    "outputId": "ec08c2ff-cf2d-4b06-e5f1-0c4c197dcd0a"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with the contestant images\n",
    "\n",
    "path = Path('contestants/')\n",
    "images = path.iterdir()\n",
    "\n",
    "contestants_df = pd.DataFrame({'Filename': [str(image) for image in images]})\n",
    "contestants_df['name'] = contestants_df['Filename'].str.split('/', expand=True)[1].str.split('_', expand=True)[0].str.split('.', expand=True)[0]\n",
    "contestants_df['Rating'] = 0\n",
    "contestants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "PCyMZNb4SZks",
    "outputId": "60bacbd8-21c7-441e-8334-147cafcaafa9"
   },
   "outputs": [],
   "source": [
    "# Get data generators for training, validation and \"testing\" sets\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    data_format='channels_first', rescale=1./255, validation_split=.1,\n",
    "    \n",
    "    # Modify images for training\n",
    "    rotation_range=20,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "def get_train_generator(df, subset):\n",
    "    return train_datagen.flow_from_dataframe(\n",
    "        df, '.', x_col='Filename', y_col='Rating',\n",
    "        target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "        class_mode='other',\n",
    "        drop_duplicates=False,\n",
    "        subset=subset\n",
    "    )\n",
    "\n",
    "train_generator = get_train_generator(train_df, 'training')\n",
    "validation_generator = get_train_generator(train_df, 'validation')\n",
    "\n",
    "test_datagen = ImageDataGenerator(data_format='channels_first', rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    contestants_df, None, x_col='Filename',\n",
    "    target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "    y_col='Rating',\n",
    "    class_mode='other',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SesD0LBShB1"
   },
   "outputs": [],
   "source": [
    "# Return a custom CNN model\n",
    "def get_custom_model():\n",
    "  input_layer = Input(shape=(3, ) + IMAGE_SIZE, name='input_layer')\n",
    "\n",
    "  conv_out = input_layer\n",
    "\n",
    "  conv_out = Conv2D(8, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  conv_out = Conv2D(16, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  conv_out = Conv2D(32, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  conv_out = Conv2D(64, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  conv_out = Conv2D(128, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  conv_out = Conv2D(256, (3, 3), data_format='channels_first', activation='relu')(conv_out)\n",
    "  conv_out = MaxPooling2D(pool_size=(2, 2), data_format='channels_first')(conv_out)\n",
    "\n",
    "  dense_out = Flatten()(conv_out)\n",
    "  dense_out = Dense(32, activation='relu')(dense_out)\n",
    "  dense_out = Dropout(.5)(dense_out)\n",
    "  dense_out = Dense(1, activation='relu')(dense_out)\n",
    "\n",
    "  model = Model(input_layer, dense_out, name=\"custom\")\n",
    "\n",
    "  model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam', metrics=['mae'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "# Return a model using the Inception Resnet v2 architecture\n",
    "def get_inception_resnet():\n",
    "  model = applications.inception_resnet_v2.InceptionResNetV2(include_top=False, input_shape=(3, ) + IMAGE_SIZE, weights='imagenet', pooling='avg')\n",
    "  dense_out = model.output\n",
    "  dense_out = Dense(1, activation='relu')(dense_out)\n",
    "  model = Model(model.input, dense_out, name=model.name)\n",
    "  model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam', metrics=['mae'])\n",
    "  return model\n",
    "\n",
    "# Return a model using the Inception V3 architecture\n",
    "def get_inception_v3():\n",
    "  model = applications.inception_v3.InceptionV3(include_top=False, input_shape=(3, ) + IMAGE_SIZE, weights='imagenet', pooling='avg')\n",
    "  dense_out = model.output\n",
    "  dense_out = Dense(1, activation='relu')(dense_out)\n",
    "  model = Model(model.input, dense_out, name=model.name)\n",
    "  model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam', metrics=['mae'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FkfnTanJxtEw",
    "outputId": "5a0e23ff-4030-4505-d230-9bf4f68e0da1"
   },
   "outputs": [],
   "source": [
    "# Train each of the models in turn then use the best weights to rate the\n",
    "# images of the contestants\n",
    "for get_model in [get_custom_model, get_inception_resnet, get_inception_v3]:\n",
    "  model = get_model()\n",
    "\n",
    "  run_name = model.name + str(model_counter)\n",
    "  model_counter += 1\n",
    "\n",
    "  filepath = str(\"%s-{epoch:02d}-{loss:.2f}.hdf5\" % run_name)\n",
    "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "  csv_logger = CSVLogger(str('%s_training_log.csv' % run_name), append=True, separator=',')\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "\n",
    "  callbacks_list = [checkpoint, csv_logger, early_stopping]\n",
    "\n",
    "  model.fit(\n",
    "    train_generator,\n",
    "    epochs=200,\n",
    "    steps_per_epoch = train_generator.n // BATCH_SIZE,\n",
    "\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.n // BATCH_SIZE,\n",
    "\n",
    "    callbacks=callbacks_list\n",
    "  )\n",
    "\n",
    "  # Load back the best weights\n",
    "  model.load_weights(str(sorted(Path('.').glob('%s*.hdf5' % run_name))[-1]))\n",
    "\n",
    "  # Run the model on the contestants' images\n",
    "  contestants_df[model.name] = model.predict_generator(\n",
    "    test_generator, \n",
    "    verbose=0, \n",
    "    steps=int(np.ceil(test_generator.n / test_generator.batch_size))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "k6e2zxgU7pmo",
    "outputId": "64a1e336-6bcd-4e7c-80a5-b6ee1ce7e0b5"
   },
   "outputs": [],
   "source": [
    "# Get the average score for each person and model, and average them out for a final Rating\n",
    "ratings_df = contestants_df.groupby('name').mean()[['inception_v3', 'inception_resnet_v2', 'custom']]\n",
    "ratings_df['Rating'] = ratings_df.mean(axis=1)\n",
    "ratings_df.sort_values('Rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ub6_3dbpwX9Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Shallow Deep Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
